<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    
    <title>Storm-Kafka Stream 卡住问题分析 | GDC-SIGMA</title>
    <meta name="renderer" content="webkit">
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <meta name="description" content="我们是一群萌萌哒的小少年">

    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Storm-Kafka Stream 卡住问题分析 | GDC-SIGMA">
    <meta name="twitter:description" content="我们是一群萌萌哒的小少年">

    <meta property="og:type" content="article">
    <meta property="og:title" content="Storm-Kafka Stream 卡住问题分析 | GDC-SIGMA">
    <meta property="og:description" content="我们是一群萌萌哒的小少年">

    
    <meta name="author" content="gdc-sigma">
    
    <link rel="stylesheet" href="/css/vno.css">
    <link rel="stylesheet" href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css">

    
    <link rel="icon" href="/images/sigma-logo.png">
    

    <meta name="generator" content="hexo"/>
    
    <link rel="alternate" type="application/rss+xml" title="GDC-SIGMA" href="/atom.xml">
    

    <link rel="canonical" href="http://yoursite.com/2017/07/29/storm-kafka-stream卡住问题分析/"/>

                 
</head>

<body class="home-template no-js">
    <script src="//cdn.bootcss.com/jquery/2.1.4/jquery.min.js"></script>
    <script src="/js/main.js"></script>
    <span class="mobile btn-mobile-menu">
        <i class="fa fa-list btn-mobile-menu__icon"></i>
        <i class="fa fa-angle-up btn-mobile-close__icon hidden"></i>
    </span>

    
<header class="panel-cover panel-cover--collapsed" style="background-image: url(/images/background-cover.jpg)">
  <div class="panel-main">
    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        <a href="/" title="前往 GDC-SIGMA 的主页"><img src="/images/sigma-logo.png" width="80" alt="GDC-SIGMA logo" class="panel-cover__logo logo" /></a>
        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage for GDC-SIGMA">GDC-SIGMA</a></h1>
        
        <span class="panel-cover__subtitle panel-subtitle">gdc-sigma的团队博客</span>
        
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description">我们是一群萌萌哒的小少年</p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />

        <div class="navigation-wrapper">
          <div>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              <li class="navigation__item"><a href="/#blog" title="访问博客" class="blog-button">博客</a></li>
            
              <li class="navigation__item"><a href="/aboutme">关于我们</a></li>
            
            </ul>
          </nav>
          </div>
          <div>
          <nav class="cover-navigation navigation--social">
  <ul class="navigation">

  <!-- Weibo-->
  

  <!-- Github -->
  
  <li class="navigation__item">
    <a href="https://github.com/gdc-sigma" title="查看我的GitHub主页" target="_blank">
      <i class='social fa fa-github'></i>
      <span class="label">Github</span>
    </a>
  </li>


<!-- Stack Overflow -->
        

  <!-- Google Plus -->
  

<!-- Facebook -->

  
<!-- Twitter -->

  

  <li class="navigation__item">
    <a href="/atom.xml" title="RSS" target="_blank">
      <i class='social fa fa-rss'></i>
      <span class="label">RSS</span>
    </a>
  </li>



  </ul>
</nav>

          </div>
        </div>

      </div>

    </div>

    <div class="panel-cover--overlay cover-slate"></div>
  </div> 
</header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            <article class="post-container post-container--single">

  <header class="post-header">
    <div class="post-meta">
      谢志旺 |
      <time datetime="2017-07-29T04:22:50.000Z" class="post-list__meta--date date">2017-07-29</time> &#8226;
      <span class="post-meta__tags tags">于&nbsp;
  <a class="tag-link" href="/tags/Kafka/">Kafka</a>, <a class="tag-link" href="/tags/Storm/">Storm</a>
</span>
      <span class="page-pv">
      &nbsp;阅读&nbsp;<span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span>
      </span> 
   
    </div>
    <h1 class="post-title">Storm-Kafka Stream 卡住问题分析</h1>
  </header>

  <section class="post">
    <p>最近刚上手storm，在进行日志业务分析的时候，使用如下的处理流：<br><img src="/images/storm-cdn.jpg" alt><br>从kafka读取数据，然后进行处理，再将处理结果写回kafka这样一个流程。在测试过程中，出现了整个流卡住的问题，解决的过程比较曲折，这里记录一下问题的原因分析。</p>
<h2 id="问题介绍"><a href="#问题介绍" class="headerlink" title="问题介绍"></a>问题介绍</h2><p>先来看看简要代码介绍：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    String[] correctArgs = parseArgs(args);</span><br><span class="line">    BaseRichBolt splitBolt;</span><br><span class="line">    BaseRichBolt normalizeBolt;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (args[<span class="number">4</span>].equals(<span class="string">"wangsu"</span>)) &#123;</span><br><span class="line">        splitBolt = <span class="keyword">new</span> CdnWangsuSplitBolt();</span><br><span class="line">        normalizeBolt =<span class="keyword">new</span> CdnWangsuNormalizeBolt();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        splitBolt = <span class="keyword">new</span> CdnAwsSplitBolt();</span><br><span class="line">        normalizeBolt = <span class="keyword">new</span> CdnAwsNormalizeBolt();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    KafkaSpout spout = getKafkaSpout(correctArgs);</span><br><span class="line">    TopologyBuilder builder = <span class="keyword">new</span> TopologyBuilder();</span><br><span class="line">    builder.setSpout(CDN_KAFKA_SPOUT_ID, spout, <span class="number">16</span>);</span><br><span class="line">    builder.setBolt(CDN_SPLIT_LOG_BOLT_ID, splitBolt, <span class="number">16</span>).shuffleGrouping(CDN_KAFKA_SPOUT_ID);</span><br><span class="line">    builder.setBolt(CDN_NORMALIZE_LOG_BOLT_ID, normalizeBolt, <span class="number">64</span>).shuffleGrouping(CDN_SPLIT_LOG_BOLT_ID);</span><br><span class="line">    builder.setBolt(CDN_AGGREATOR_LOG_BOLT_ID, <span class="keyword">new</span> CdnLogAggreator(), <span class="number">128</span>).fieldsGrouping(CDN_NORMALIZE_LOG_BOLT_ID, <span class="keyword">new</span> Fields(<span class="string">"key"</span>));</span><br><span class="line">    builder.setBolt(CDN_OUTPUR_LOG_BOLT_ID, <span class="keyword">new</span> KafkaOutputBolt().getKafkaBolt(args[<span class="number">1</span>], args[<span class="number">3</span>]), <span class="number">16</span>).shuffleGrouping(CDN_AGGREATOR_LOG_BOLT_ID);</span><br><span class="line">    </span><br><span class="line">    LOG.info(<span class="string">"brokers: "</span> + args[<span class="number">1</span>] + <span class="string">" output: "</span> + args[<span class="number">3</span>]);</span><br><span class="line">    StormSubmitter.submitTopology(correctArgs[<span class="number">5</span>], getTopologyConfig(correctArgs), builder.createTopology());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>其中 splitBolt 的代码为：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CdnWangsuSplitBolt</span> <span class="keyword">extends</span> <span class="title">BaseRichBolt</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Logger LOG = <span class="keyword">new</span> MyLogger().getLogger(CdnWangsuSplitBolt<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String SITENAME = <span class="string">"wangsu"</span>;</span><br><span class="line">    <span class="keyword">private</span> LogParser logParser;</span><br><span class="line">    <span class="keyword">private</span> OutputCollector collector;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">prepare</span><span class="params">(Map config, TopologyContext context, OutputCollector collector)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.collector = collector;</span><br><span class="line">        <span class="keyword">this</span>.logParser = <span class="keyword">new</span> LogParser(SITENAME);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">declareOutputFields</span><span class="params">(OutputFieldsDeclarer declarer)</span> </span>&#123;</span><br><span class="line">        declarer.declare(<span class="keyword">new</span> Fields(<span class="string">"clientip"</span>, <span class="string">"time"</span>, <span class="string">"method"</span>, <span class="string">"host"</span>, <span class="string">"path"</span>, <span class="string">"protocol"</span>,</span><br><span class="line">                <span class="string">"protocol-version"</span>, <span class="string">"code"</span>, <span class="string">"size"</span>, <span class="string">"dltime"</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(Tuple tuple)</span> </span>&#123;</span><br><span class="line">        String logstr = tuple.getString(<span class="number">0</span>);</span><br><span class="line">        Map&lt;String, String&gt; logEntry = <span class="keyword">this</span>.logParser.initLogMapEntry(logstr);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (logEntry == <span class="keyword">null</span>) &#123;</span><br><span class="line">            LOG.warning(<span class="string">"Invalid log: "</span> + logstr);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        String clientip = logEntry.get(<span class="string">"clientip"</span>);</span><br><span class="line">        String time = logEntry.get(<span class="string">"time"</span>);</span><br><span class="line">        String method = logEntry.get(<span class="string">"method"</span>);</span><br><span class="line">        String host = logEntry.get(<span class="string">"channel"</span>);</span><br><span class="line">        String path = logEntry.get(<span class="string">"url"</span>);</span><br><span class="line">        String protocol_version = logEntry.get(<span class="string">"protocol-version"</span>);</span><br><span class="line">        String protocol = logEntry.get(<span class="string">"protocol"</span>);</span><br><span class="line">        String code = logEntry.get(<span class="string">"code"</span>);</span><br><span class="line">        String size = logEntry.get(<span class="string">"size"</span>);</span><br><span class="line">        String dltime = logEntry.get(<span class="string">"dltime"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (code.equals(<span class="string">"0"</span>)) &#123;</span><br><span class="line">            LOG.warning(<span class="string">"Invalid log: "</span> + logstr);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">this</span>.collector.emit(tuple, <span class="keyword">new</span> Values(clientip, time, method, host, path, protocol,</span><br><span class="line">                protocol_version, code, size, dltime));</span><br><span class="line">        collector.ack(tuple);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>出现的问题是，集群每次跑到一定的程度就卡在那里了，例如上面的配置，跑到1千多万条数据的时候，就开始卡在那里了，从监控的流量图来看如下：<br>￼<img src="/images/storm-cdn-monitor.png" alt><br>可以看到，kafka的数据消费速度基本没变，说明kafka-spout是基本处于正常状态的，但是数据的产生到后面确实越来越低主键变为0。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>首先说一下解决方案：</p>
<ol>
<li>将BaseRichBolt换成BaseBasicBolt</li>
<li>在目前的实现里面，executor严格进行<code>ack</code>，例如改为如下：<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CdnWangsuSplitBolt</span> <span class="keyword">extends</span> <span class="title">BaseRichBolt</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(Tuple tuple)</span> </span>&#123;</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (logEntry == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> Exception(<span class="string">"Invalid log: "</span> + logstr);</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            String clientip = logEntry.get(<span class="string">"clientip"</span>);</span><br><span class="line">            String time = logEntry.get(<span class="string">"time"</span>);</span><br><span class="line">            String method = logEntry.get(<span class="string">"method"</span>);</span><br><span class="line">            String host = logEntry.get(<span class="string">"channel"</span>);</span><br><span class="line">            String path = logEntry.get(<span class="string">"url"</span>);</span><br><span class="line">            String protocol_version = logEntry.get(<span class="string">"protocol-version"</span>);</span><br><span class="line">            String protocol = logEntry.get(<span class="string">"protocol"</span>);</span><br><span class="line">            String code = logEntry.get(<span class="string">"code"</span>);</span><br><span class="line">            String size = logEntry.get(<span class="string">"size"</span>);</span><br><span class="line">            String dltime = logEntry.get(<span class="string">"dltime"</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (code.equals(<span class="string">"0"</span>)) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> Exception(<span class="string">"Invalid log: "</span> + logstr);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">this</span>.collector.emit(<span class="keyword">new</span> Values(clientip, time, method, host, path, protocol,</span><br><span class="line">                    protocol_version, code, size, dltime));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            LOG.warning(e.getMessage());</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            collector.ack(tuple);</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>我们先不管代码的写法是不是最佳实践（因为一直写python，最近才开始写java，还不是很熟悉），我想强调的是，不管每个tuple你是怎么处理的，在这里对于无效日志不能直接跳过，一定要进行<code>ack</code>。</p>
<h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>那么来看一下这里为什么不进行ack就会出现stream卡住的问题，首先可以看到我这里使用的是默认storm配置，也就是30s没有进行ack就会被认为消息处理失败，从而会调用kafka-spout的<code>fail</code>方法进行处理，那么我们来看看kafka-spout的<code>fail</code>是怎么写的：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">fail</span><span class="params">(Object msgId)</span> </span>&#123;</span><br><span class="line">    KafkaMessageId id = (KafkaMessageId) msgId;</span><br><span class="line">    PartitionManager m = _coordinator.getManager(id.partition);</span><br><span class="line">    <span class="keyword">if</span> (m != <span class="keyword">null</span>) &#123;</span><br><span class="line">        m.fail(id.offset);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>可以看到，使用了<code>PartitionManager</code>来进行管理，那么我们直接看看<code>PartitionManager</code>的<code>fail</code>实现：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">fail</span><span class="params">(Long offset)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (offset &lt; _emittedToOffset - _spoutConfig.maxOffsetBehind) &#123;</span><br><span class="line">        LOG.info(...);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        LOG.debug(<span class="string">"Failing at offset=&#123;&#125; with _pending.size()=&#123;&#125; pending and _emittedToOffset=&#123;&#125; for &#123;&#125;"</span>, offset, _pending.size(), _emittedToOffset, _partition);</span><br><span class="line">        numberFailed++;</span><br><span class="line">        <span class="keyword">if</span> (numberAcked == <span class="number">0</span> &amp;&amp; numberFailed &gt; _spoutConfig.maxOffsetBehind) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Too many tuple failures"</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Offset may not be considered for retry by failedMsgRetryManager</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>._failedMsgRetryManager.retryFurther(offset)) &#123;</span><br><span class="line">            <span class="keyword">this</span>._failedMsgRetryManager.failed(offset);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// state for the offset should be cleaned up</span></span><br><span class="line">            LOG.warn(<span class="string">"Will not retry failed kafka offset &#123;&#125; further"</span>, offset);</span><br><span class="line">            _messageIneligibleForRetryCount.incr();</span><br><span class="line">            _pending.remove(offset);</span><br><span class="line">            <span class="keyword">this</span>._failedMsgRetryManager.acked(offset);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>注意：</strong></p>
<blockquote>
<p>这里的 _spoutConfig.maxOffsetBehind使用的是默认配置，值为 Long.MAX_VALUE，在64位机器上是9223372036854775807</p>
</blockquote>
<p>我们这里先不进行摄入研究，但是从最上层的is-else可以看出，在失败消息条数未达到设置的上限（Long.MAX_VALUE）时，如果消息发送失败，就会重试。到这里，我们可以看到一个问题，那就是如果<code>maxOffsetBehind</code>设置得比较大，那么会出现<code>failed</code>的消息永远不会被忽略，而会一直重试直到成功。这个基本解释了为什么前面的代码里面一定要加上<code>ack</code>，如果不加上，会导致<code>failed</code>的消息一直占用内存，同时占用计算资源。（关于fail更深入的分析在后面进行）<br>但是这不能解释另一个问题，如果只是上面这部分代码，虽然有<code>failed</code>的信息会占用资源，但是从kafka-spout的执行情况来看（没有截图），<code>failed</code>的消息也就不到10w条，还不至于阻塞住整个stream，并且从jvm的GC和监控里面机器内存消耗来看，其实内存上是没有什么影响的。</p>
<p>那么我们换个角度，从kafka-spout的<code>nextTuple</code>的实现来看，它是调PartitionManager的next来进emit消息的：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EmitState state = managers.get(_currPartitionIndex).next(_collector);</span><br></pre></td></tr></table></figure></p>
<p>那么我们来看看PartitionManager的next方法：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> EmitState <span class="title">next</span><span class="params">(SpoutOutputCollector collector)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (_waitingToEmit.isEmpty()) &#123;</span><br><span class="line">        fill();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        MessageAndOffset toEmit = _waitingToEmit.pollFirst();</span><br><span class="line">        ...</span><br><span class="line">        Iterable&lt;List&lt;Object&gt;&gt; tups;</span><br><span class="line">        <span class="keyword">if</span> (_spoutConfig.scheme <span class="keyword">instanceof</span> MessageMetadataSchemeAsMultiScheme) &#123;</span><br><span class="line">            tups = KafkaUtils.generateTuples((MessageMetadataSchemeAsMultiScheme) _spoutConfig.scheme, toEmit.message(), _partition, toEmit.offset());</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            tups = KafkaUtils.generateTuples(_spoutConfig, toEmit.message(), _partition.topic);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> ((tups != <span class="keyword">null</span>) &amp;&amp; tups.iterator().hasNext()) &#123;</span><br><span class="line">           <span class="keyword">if</span> (!Strings.isNullOrEmpty(_spoutConfig.outputStreamId)) &#123;</span><br><span class="line">                <span class="keyword">for</span> (List&lt;Object&gt; tup : tups) &#123;</span><br><span class="line">                    collector.emit(_spoutConfig.topic, tup, <span class="keyword">new</span> KafkaMessageId(_partition, toEmit.offset()));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">for</span> (List&lt;Object&gt; tup : tups) &#123;</span><br><span class="line">                    collector.emit(tup, <span class="keyword">new</span> KafkaMessageId(_partition, toEmit.offset()));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            ack(toEmit.offset());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>去掉了一些非重点的部分，我们看这个方法的实现可以看出，在while里面会不断从<code>_waitingToEmit</code>获取消息，然后通过<code>KafkaUtils</code>构造新的tuple进行发射（<code>_waitingToEmit</code> 表示所有己经被从kafka读取，但是还没有发射到topology流中的消息）。当next被调用的时候，它只会从_waitingToEmit获取消息，如果_waitingToEmit为空，就会调用fill方法。那么我们来看看fill方法到底干了啥。</p>
<p>fill方法主要逻辑分为以下三部分：</p>
<ol>
<li>判断应该从哪个offset开始获取消息</li>
<li>获取消息，处理TopicOffsetOutOfRangeException异常</li>
<li>把获取的消息放到_waitingToEmit中，同时结合failed集合和pendding集合进行处理<br>我们分别来介绍着三部分：<h3 id="第一部分在：找到offset"><a href="#第一部分在：找到offset" class="headerlink" title="第一部分在：找到offset"></a>第一部分在：找到offset</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Are there failed tuples? If so, fetch those first.</span></span><br><span class="line">offset = <span class="keyword">this</span>._failedMsgRetryManager.nextFailedMessageToRetry();</span><br><span class="line"><span class="keyword">final</span> <span class="keyword">boolean</span> processingNewTuples = (offset == <span class="keyword">null</span>);</span><br><span class="line"><span class="keyword">if</span> (processingNewTuples) &#123;</span><br><span class="line">    offset = _emittedToOffset;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>这段代码里，offset即是将要从Kafka里抓取消息的offset。当failed集合不为空时，就用failed集合的最小的offset做为下次要抓取的offset（默认的failed处理类<code>ExponentialBackoffMsgRetryManager</code>的处理方案）。Kafka的FetchRequest每次会从Kafka中获取一批消息。所以，如果有消息fail，而此failed消息之后的消息已被ack，那么fill方法会重新获取这些已被ack的消息，如果不对这些消息进行过滤，就会造成重复消费问题，我们后面会看到，fill方法是会进行处理的。<br>如果没有failed消息，fill方法就会从之前读取过的最大的offset开始继续读取。</p>
<h3 id="第二部分：获取消息"><a href="#第二部分：获取消息" class="headerlink" title="第二部分：获取消息"></a>第二部分：获取消息</h3><p>知道了从哪里开始获取消息后，接下来就开始获取：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    msgs = KafkaUtils.fetchMessages(_spoutConfig, _consumer, _partition, offset);</span><br><span class="line">&#125; <span class="keyword">catch</span> (TopicOffsetOutOfRangeException e) &#123;</span><br><span class="line">    offset = KafkaUtils.getOffset(_consumer, _partition.topic, _partition.partition, kafka.api.OffsetRequest.EarliestTime());</span><br><span class="line">    <span class="comment">// fetch failed, so don't update the fetch metrics</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">//fix bug [STORM-643] : remove outdated failed offsets</span></span><br><span class="line">    ... 这里是处理另一个bug的，先忽略</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这部分的分析涉及到offset的异常处理（要读取的offset不在kafka能提供的offset范围内），不是我们这里要讨论的问题，所以先跳过。暂时只将他们简化为消息获取部分。</p>
<h3 id="第三部分：消息处理"><a href="#第三部分：消息处理" class="headerlink" title="第三部分：消息处理"></a>第三部分：消息处理</h3><p>获取到消息后，需要处理各种例外情况：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (MessageAndOffset msg : msgs) &#123;</span><br><span class="line">    <span class="keyword">final</span> Long cur_offset = msg.offset();</span><br><span class="line">    <span class="keyword">if</span> (cur_offset &lt; offset) &#123;</span><br><span class="line">        <span class="comment">// Skip any old offsets.</span></span><br><span class="line">        <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (processingNewTuples || <span class="keyword">this</span>._failedMsgRetryManager.shouldReEmitMsg(cur_offset)) &#123;</span><br><span class="line">        numMessages += <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (!_pending.containsKey(cur_offset)) &#123;</span><br><span class="line">            _pending.put(cur_offset, System.currentTimeMillis());</span><br><span class="line">        &#125;</span><br><span class="line">        _waitingToEmit.add(msg);</span><br><span class="line">        _emittedToOffset = Math.max(msg.nextOffset(), _emittedToOffset);</span><br><span class="line">        <span class="keyword">if</span> (_failedMsgRetryManager.shouldReEmitMsg(cur_offset)) &#123;</span><br><span class="line">            <span class="keyword">this</span>._failedMsgRetryManager.retryStarted(cur_offset);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">_fetchAPIMessageCount.incrBy(numMessages);</span><br></pre></td></tr></table></figure></p>
<ul>
<li>首先，需要考虑到FetchRequest指定的是返回集合中最小的offset A，但是实际上kafka只保证返回的消息集中包括了offset为A的消息，这个消息集中可能包括了比A更小的消息（由于压缩），所以fill方法首先要skip掉这些offset更小的消息</li>
<li>如果fiiled为空（processingNewTuples），fill就会把所有offset从A开始的消息加入_waitingToEmit集合</li>
<li><p>如果failed不为空，那么遍历msgs，如果msg在failed集合里，首先把这条消息加入_waitingToEmit集合与_pending集合，同时把它从failed集合中去掉（否则这条消息就会永远在failed集合里）。注意，只有在fill方法中，failed集合中的元素才可能被移除，加入到_waitingToEmit集合，使它有机会被重新emit。其中的<code>shouldReEmitMsg</code>方法是在<code>ExponentialBackoffMsgRetryManager</code>中实现的，我们简单看一下它的实现：</p>
  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">shouldReEmitMsg</span><span class="params">(Long offset)</span> </span>&#123;</span><br><span class="line">    MessageRetryRecord record = <span class="keyword">this</span>.records.get(offset);</span><br><span class="line">    <span class="keyword">return</span> record != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">            <span class="keyword">this</span>.waiting.contains(record) &amp;&amp;</span><br><span class="line">            System.currentTimeMillis() &gt;= record.retryTimeUTC;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>  这个方法主要用来判断对应offset的消息是否在failed集合里面，如果不在，说明消息已经被成功消费过了，因此直接跳过这条消息。</p>
</li>
</ul>
<p>从上面的结果可以看出，如果failed的消息会一直失败，这个task就会一直卡在处理失败消息这部地方。在我们的场景里面，会有一些日志不符合要求，尤其是上面的第二个过滤条件：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (code.equals(<span class="string">"0"</span>)) &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这个时候我们是直接return的，这就导致这些tuple没有ack，而这种写法里面，对于无效日志是直接返回，永远不会被ack的，也就会不断重试。也就是，task会不断从第一个失败的offset开始从kafka获取相同的一批数据，然后处理这批数据里面的无效日志。这也就解释了我前面给出的监控图里面，网卡流量中读的流量一直居高不下，但是写的流量逐渐减少到最后完全不写了。这是因为各个task都逐渐被卡住了。</p>
<p>到这里，或许会有一个疑问，前面说到，根据<code>shouldReEmitMsg</code>方法返回值来判断一个消息是否是failed的，如果是，那么才会处理，那么如果这一批msgs中有消息是没有处理过的呢（例如在失败消息第一次处理的之后才插入kafka，还没有被消费的），那不是被跳过了？其实不会的，我们看以下代码（前面第三部分消息处理部分里的）：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (processingNewTuples || <span class="keyword">this</span>._failedMsgRetryManager.shouldReEmitMsg(cur_offset)) &#123;</span><br><span class="line">    ...</span><br><span class="line">    _emittedToOffset = Math.max(msg.nextOffset(), _emittedToOffset);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>可以看到，在处理failed的消息的时候，会将_emittedToOffset设置为当前已处理的消息里面最新的offset的，那么在下一次获取消息时，如果已经没有failed了的消息了，那么就会从_emittedToOffset开始获取消息，因此可以保证不重复消费消息，也不漏掉消息。</p>
<h2 id="其他分析"><a href="#其他分析" class="headerlink" title="其他分析"></a>其他分析</h2><p>前面我们基本通过源码搞清楚了kafka-spout的失败消息处理原则，也搞清楚了我们出现stream卡住的问题的原因。下面进一步分析一下PartitionManager的ack和fail方法。<br>我们先来看ack方法：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">ack</span><span class="params">(Long offset)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!_pending.isEmpty() &amp;&amp; _pending.firstKey() &lt; offset - _spoutConfig.maxOffsetBehind) &#123;</span><br><span class="line">        <span class="comment">// Too many things pending!</span></span><br><span class="line">        _pending.headMap(offset - _spoutConfig.maxOffsetBehind).clear();</span><br><span class="line">    &#125;</span><br><span class="line">    _pending.remove(offset);</span><br><span class="line">    <span class="keyword">this</span>._failedMsgRetryManager.acked(offset);</span><br><span class="line">    numberAcked++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>ack的主要功能是把成功了的消息从_pending去掉，表示这个消息处理完成，PartitionManager根据这个获取正确的处理进度信息，以更新zk里面的offset记录。同时删除可能存在以failed集合中的对应记录。但是，他还有另一个作用，也就是这句：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (!_pending.isEmpty() &amp;&amp; _pending.firstKey() &lt; offset - _spoutConfig.maxOffsetBehind) &#123;</span><br><span class="line">    <span class="comment">// Too many things pending!</span></span><br><span class="line">    _pending.headMap(offset - _spoutConfig.maxOffsetBehind).clear();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>当一个offset被ack时，ack方法会把所有小于offset - _spoutConfig.maxOffsetBehind的消息从_pending中移除。也就是说，即使这些被移除的消息失败了，也认为他们处理成功，使得在Zookeeper中记录的进度忽略这些被移除的消息。所以，假如task重启，那么这些失败但被移除出_pending集合的消息就不会被再处理。所以在设置maxOffsetBehind的时候需要考虑好这个问题。<br>那么，这些失败了的消息，当storm的acker发现它们处理失败了，会发生什么呢？这个由fail方法决定。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">fail</span><span class="params">(Long offset)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (offset &lt; _emittedToOffset - _spoutConfig.maxOffsetBehind) &#123;</span><br><span class="line">        LOG.info(...);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        LOG.debug(<span class="string">"Failing at offset=&#123;&#125; with _pending.size()=&#123;&#125; pending and _emittedToOffset=&#123;&#125; for &#123;&#125;"</span>, offset, _pending.size(), _emittedToOffset, _partition);</span><br><span class="line">        numberFailed++;</span><br><span class="line">        <span class="keyword">if</span> (numberAcked == <span class="number">0</span> &amp;&amp; numberFailed &gt; _spoutConfig.maxOffsetBehind) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Too many tuple failures"</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Offset may not be considered for retry by failedMsgRetryManager</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>._failedMsgRetryManager.retryFurther(offset)) &#123;</span><br><span class="line">            <span class="keyword">this</span>._failedMsgRetryManager.failed(offset);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// state for the offset should be cleaned up</span></span><br><span class="line">            LOG.warn(<span class="string">"Will not retry failed kafka offset &#123;&#125; further"</span>, offset);</span><br><span class="line">            _messageIneligibleForRetryCount.incr();</span><br><span class="line">            _pending.remove(offset);</span><br><span class="line">            <span class="keyword">this</span>._failedMsgRetryManager.acked(offset);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>当一个消息对应的tuple被fail时，fail方法首先会判断这个消息是否落后太多。如果它的offset小于（当前读取的最大offset-_spoutConfig.maxOffsetBehind），那么就不把它加到failed集合里，使得它不会被重新处理。如果不落后太多，就把它加到failed集合（所以在消息的一致性要求不高的时候，可以通过maxOffsetBehind来减少fail消息对集群的影响）。<br>如果还没有消息被ack，并且失败数量太多（numberFailed &gt; _spoutConfig.maxOffsetBehind），就会抛异常，表示PartitionManager工作出错。而这种情况只有在处理第一批消息并且这批消息个数大于maxOffsetBehind才行。<br>如果前面的条件都没有满足，流程走到了最后一个if-lse（也是大部分消息的处理流程会走到的部分），首先会通过<code>_failedMsgRetryManager.retryFurther(offset)</code>判断这条消息是否还需要重试，如果是，把它加到failed队列，否则将其从failed（如果存在）队列删除，也就是不再处理这条消息。我们来看retryFurther的实现：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">retryFurther</span><span class="params">(Long offset)</span> </span>&#123;</span><br><span class="line">    MessageRetryRecord record = <span class="keyword">this</span>.records.get(offset);</span><br><span class="line">    <span class="keyword">return</span> ! (record != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">           <span class="keyword">this</span>.retryLimit &gt; <span class="number">0</span> &amp;&amp;</span><br><span class="line">           <span class="keyword">this</span>.retryLimit &lt;= record.retryNum);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>可以看到，判断逻辑为：</p>
<ul>
<li>如果消息还没有重试过（不在failed集合），那么返回true</li>
<li>如果消息存在，那么需要retryLimit<0 || retrylimit> record.retryNum<br>也就是说，可以通过retryLimit（默认为-1）来控制消息的重试次数，所以也可以通过这个参数来避免failed的消息死循环消费问题。</0></li>
</ul>

  </section>

</article>

<section class="read-more">
           
    
               
            <div class="read-more-item">
                <span class="read-more-item-dim">最近的文章</span>
                <h2 class="post-list__post-title post-title"><a href="/2017/08/12/go-runtime-intro/" title="Go Runtime 浅析">Go Runtime 浅析</a></h2>
                <p class="excerpt">
                
                在 GDC Sigma 小组为期两个月的实习已告一段落，本人也十分有幸能在实习期间在团队内部完成了三次 Go 语言相关的分享。由于个人的不足，很遗憾没能向组员分享更多深入的内容，但尽管内容粗浅，这三次分享仍在组内起到了很好的科普作用，收获了组员的一致好评。受组员委托，我将把这三次分享上与 Go 运行
                &hellip;
                </p>
                <div class="post-list__meta"><time datetime="2017-08-11T16:00:00.000Z" class="post-list__meta--date date">2017-08-12</time> &#8226; <span class="post-list__meta--tags tags">于&nbsp;
  <a class="tag-link" href="/tags/Go/">Go</a>
</span><a class="btn-border-small" href="/2017/08/12/go-runtime-intro/">继续阅读</a></div>
                           
            </div>
        
        
        
     
   
   
  
</section>

  
<section class="post-comments">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
      var disqus_shortname = 'www-gdc-sigma-com'; 
      
  var disqus_identifier = '/2017/07/29/storm-kafka-stream卡住问题分析/';
  var disqus_title = 'Storm-Kafka Stream 卡住问题分析';
  var disqus_url = 'http://yoursite.com/2017/07/29/storm-kafka-stream卡住问题分析/';
  

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          //dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
          dsq.src = 'https://a.disquscdn.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



            <footer class="footer">
    <span class="footer__copyright">
        本站点采用 <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>
    </span>
    <span class="footer__copyright">
        基于 <a href="http://hexo.io">Hexo</a> 搭建，感谢 <a href="https://pages.github.com/">GitHub Pages</a> 提供免费的托管服务
    </span>
    <span class="footer__copyright">
        &copy; 2021 - 本站使用 <a href="https://github.com/monniya/hexo-theme-new-vno ">new-vno</a> 主题,
        由<a href="https://monniya.com ">@Monniya</a> 修改自 <a href="https://github.com/lenbo-ma/hexo-theme-vno" target="_blank">Vno</a>, 原创出自<a href="http://github.com/onevcat/vno" target="_blank">onevcat</a>
    </span>
    
</footer>


        </div>
    </div>

     
    


    <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
    
</body>
</html>
